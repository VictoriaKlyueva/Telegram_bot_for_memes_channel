import telebot

import os
import numpy as np

import torch
from transformers import pipeline
from diffusers import DDPMScheduler, UNet2DModel
from ultralytics import YOLO

from PIL import ImageFont, ImageDraw, Image
from io import BytesIO

import random
import codecs

CHANNEL_ID = -1002242343472
TIME_STEPS = 1000
IMAGE_SIZE = 256
MY_CHAT_ID = 748487218
HF_USERNAME = "vikosik3000"
HF_MODEL_NAME = "rugpt2-memes-finetuned"
TOKENIZER_PATH = "ai-forever/rugpt3small_based_on_gpt2"
EVAL_BATCH_SIZE = 6  # прощай видюха


def import_token(path):
    with open(os.path.join(path, 'token.txt')) as f:
        token = f.read().strip()

    return token


# Make bot session
token = import_token('')
bot = telebot.TeleBot(token)

# Load diffusion model and scheduler
scheduler = DDPMScheduler.from_pretrained("google/ddpm-cat-256")
model = UNet2DModel.from_pretrained("google/ddpm-cat-256").to('cuda')
scheduler.set_timesteps(TIME_STEPS)

upload_directory = "local_models/diffuser"
model.from_pretrained(upload_directory).to('cuda')
scheduler.from_pretrained(upload_directory)

print("Диффузионная модель загружена")

# Load yolo for generations validation
yolo = YOLO("yolo11n.pt")

print("YOLO загружен")

# Making pipeline for transformer
pipe = pipeline(
    'text-generation',
    model=f"{HF_USERNAME}/{HF_MODEL_NAME}",
    tokenizer=TOKENIZER_PATH,
    temperature=1.2,
    top_k=50,
    top_p=0.9,
    do_sample=True,
    max_new_tokens=55,
    truncation=True
)

print("GPT2 загружена")

print("Все модели загружены")


def soft_max(output):
    exp_values = np.exp(output)
    return (exp_values / np.sum(exp_values))[1]


def check_image(image):
    # define preprocess parameters
    results = yolo(image)

    print(results[0].boxes.cls, results[0].boxes.conf)

    return any(
        cls == 15 and conf > 0.7
        for cls, conf in zip(results[0].boxes.cls, results[0].boxes.conf)
    )


# Get image generated by diffuser
def generate_image():
    # Generate images while there is no cat on it
    while True:
        # Generate random noise for 4 images
        random_noise = torch.randn((EVAL_BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)).to('cuda')
        model_input = random_noise

        count = 0
        for t in scheduler.timesteps:
            with torch.no_grad():
                # Process batch of images
                noisy_residual = model(model_input, t).sample
                prev_noisy_sample = scheduler.step(noisy_residual, t, model_input).prev_sample
                model_input = prev_noisy_sample

            if count % 100 == 0:
                print(f'Шаг: {count} / {scheduler.timesteps[0]}')

            count += 1

        # Post-processing
        image_batch = (model_input / 2 + 0.5).clamp(0, 1)
        image_batch = image_batch.cpu().permute(0, 2, 3, 1).numpy()  # shape: (EVAL_BATCH_SIZE, 256, 256, 3)

        # evaluate images
        for i in range(EVAL_BATCH_SIZE):
            image = image_batch[i]
            image = Image.fromarray((image * 255).round().astype("uint8"))

            # Check if there is a cat on the image
            if check_image(image):
                return image
            else:
                path = f'generated_data/bad_image_{i}.png'
                save_image(image, path)
                print(f"Фото отклонено для изображения {i}")


def save_image(image, path):
    # Save the PIL image in the root folder
    image.save(path)


def text_post_processing(text):
    return text.replace('\n', '').replace('&nbsp;', ' ').split('  ')


def get_prompt():
    path = 'prompts_data/'
    prompts_object = codecs.open(path + "prompts_ideas.txt", "r", "utf_8_sig")
    prompts = prompts_object.read().replace('\r', '').split('\n')

    prompt = random.choice(prompts).split()
    prompt = prompt if len(prompt) <= 3 else prompt[:random.randint(1, 3)]
    prompt = ' '.join(prompt)

    if len(prompt) == 1:
        addition_object = codecs.open(path + "prompts_ideas.txt", "r", "utf_8_sig")
        # additions = addition_object.read().replace('\r', '').split('\n')
        addition = random.choice(prompts)

        prompt += ' ' + addition

    return prompt


def generate_text():
    while True:
        prompt = get_prompt()
        result = random.choice(text_post_processing(pipe(prompt)[0]['generated_text'])[:2])

        if len(result) > len(prompt):
            break
        else:
            print("Генерация продолжается")

    print("Prompt:", prompt, "Output:", result)

    if len(result) > 20 and len(result.split()) == 1:
        result = result[:20]

    return result


def add_shadow(text, draw, font, x, y, offset=3, shadow_color='black'):
    for off in range(offset):
        draw.text((x - off, y), text, font=font, fill=shadow_color)
        draw.text((x + off, y), text, font=font, fill=shadow_color)
        draw.text((x, y + off), text, font=font, fill=shadow_color)
        draw.text((x, y - off), text, font=font, fill=shadow_color)
        draw.text((x - off, y + off), text, font=font, fill=shadow_color)
        draw.text((x + off, y + off), text, font=font, fill=shadow_color)
        draw.text((x - off, y - off), text, font=font, fill=shadow_color)
        draw.text((x + off, y - off), text, font=font, fill=shadow_color)


def dynamic_text_position(text):
    # Select font size
    font_size = int(480 / len(text))

    # Select left-bottom of the text
    x = 15
    y = 240 - font_size

    return font_size, x, y


def put_text_on_image(image, text):
    draw = ImageDraw.Draw(image)

    # Select font
    fonts = os.listdir('fonts')
    font_choice = random.choice(fonts)

    # Select color
    color = (255, 255, 255)

    font_size, x, y = dynamic_text_position(text)

    # Import font
    with open('fonts/' + font_choice, "rb") as f:
        bytes_font = BytesIO(f.read())
    font = ImageFont.truetype(bytes_font, font_size if font_choice == "lobster.ttf" else font_size - 1)

    # Draw shadow
    add_shadow(text, draw, font, x, y)
    # Draw text
    draw.text((x, y), text, color, font=font)

    return image


@bot.message_handler(content_types=['text', 'image'])
def send_meme(message):
    if message.from_user.username is not None:
        # Skip if start message
        if message.text == '/start':
            return

        # Print info about new generation request into console
        print("Сообщение от пользователя:", message.from_user.username)
        print("Текст сообщения::", message.text)

        # Print info about new generation request into my tg chat
        bot.send_message(MY_CHAT_ID, f'Сообщение: {message.text}')
        bot.send_message(MY_CHAT_ID, f'От пользователя: {message.from_user.username}')

        # Generate image
        print('Начата генерация фото')
        # image = Image.fromarray(np.random.randint(0, 256, (IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8))
        image = generate_image()
        print('Фото сгенерировано')

        # Generate text
        print('Начата генерация текста')
        text = generate_text()
        print('Текст сгенерирован')

        # Create meme
        meme = put_text_on_image(image, text)

        # Save meme
        path = 'generated_data/generated_meme.png'
        save_image(meme, path)
        # save_image(image, path)
        print('Мем создан')

        # Send meme
        try:
            file = open(path, 'rb')
            try:
                if message.text != 'мем' and message.text != 'Мем':
                    bot.send_photo(MY_CHAT_ID, file, caption=message.text)
                else:
                    bot.send_photo(MY_CHAT_ID, file)
                print("Мем отправлен")
            except Exception as e:
                print("Error sending the photo")
                print(e)
        except Exception as e:
            print("Error reading the file")
            print(e)


# Loop for code execution
bot.infinity_polling(timeout=10, long_polling_timeout=5)
