import telebot

import os
import numpy as np

import torch
from transformers import pipeline
from diffusers import DDPMScheduler, UNet2DModel

from PIL import ImageFont, ImageDraw, Image
from io import BytesIO
import cv2

import random
import codecs

CHANNEL_ID = -1002242343472
TIME_STEPS = 1000
IMAGE_SIZE = 256
MY_CHAT_ID = 748487218
HF_USERNAME = "vikosik3000"
HF_MODEL_NAME = "rugpt2-memes-finetuned"
TOKENIZER_PATH = "ai-forever/rugpt3small_based_on_gpt2"

def import_token(path):
    with open(os.path.join(path, 'token.txt')) as f:
        token = f.read().strip()

    return token


# Make bot session
token = import_token('')
bot = telebot.TeleBot(token)

# Load diffusion model and scheduler
scheduler = DDPMScheduler.from_pretrained("google/ddpm-cat-256")
model = UNet2DModel.from_pretrained("google/ddpm-cat-256").to('cuda')
scheduler.set_timesteps(TIME_STEPS)

upload_directory = "local_models/diffuser"
model.from_pretrained(upload_directory).to('cuda')
scheduler.from_pretrained(upload_directory)

print("Диффузионная модель загружена")

# Load resnet for generations validation
resnet = cv2.dnn.readNetFromONNX('local_models/resnet34.onnx')

print("Resnet загружен")

# Making pipeline for transformer
pipe = pipeline(
    'text-generation',
    model=f"{HF_USERNAME}/{HF_MODEL_NAME}",
    tokenizer=TOKENIZER_PATH,
    temperature=1.2,
    top_k=50,
    top_p=0.9,
    do_sample=True,
    max_new_tokens=55,
    truncation=True
)

print("GPT2 загружена")

print("Все модели загружены")


def soft_max(output):
    exp_values = np.exp(output)
    return (exp_values / np.sum(exp_values))[1]


def check_image(image):
    # define preprocess parameters
    mean = np.array([0.485, 0.456, 0.406]) * 255.0
    scale = 1 / 255.0

    cv2_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    blob = cv2.dnn.blobFromImage(
        cv2_image,
        scalefactor=scale,
        size=(IMAGE_SIZE, IMAGE_SIZE),
        mean=mean,
        swapRB=True
    )

    resnet.setInput(blob)
    output = resnet.forward()

    probability = soft_max(output[0])

    threshold = 0.85
    predicted_class = 1 if probability > threshold else 0

    print(f'Вероятность положительного класса: {probability:.4f}')

    return predicted_class


# Get image generated by diffuser
def generate_image():
    # Generate images while there is no cat on it
    while True:
        random_noise = torch.randn((1, 3, IMAGE_SIZE, IMAGE_SIZE))
        model_input = random_noise.to('cuda')

        count = 0
        for t in scheduler.timesteps:
            with torch.no_grad():
                noisy_residual = model(model_input, t).sample
                prev_noisy_sample = scheduler.step(noisy_residual, t, model_input).prev_sample
                model_input = prev_noisy_sample

            if count % 100 == 0:
                print(f'Шаг: {count} / {scheduler.timesteps[0]}')

            count += 1

        # Post-processing
        image = (model_input / 2 + 0.5).clamp(0, 1)
        image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
        image = Image.fromarray((image * 255).round().astype("uint8"))

        # upscale image
        # image = sf_pipeline(prompt="cat", image=image).images[0]

        # check if there is cat on image
        if check_image(image):
            return image

        print("Фото отклонено")


def save_image(image, path):
    # Save the PIL image in the root folder
    image.save(path)


def text_post_processing(text):
    return text.replace('\n', '').replace('&nbsp;', ' ').split('  ')


def get_prompt():
    path = 'prompts_data/'
    prompts_object = codecs.open(path + "prompts_ideas.txt", "r", "utf_8_sig")
    prompts = prompts_object.read().replace('\r', '').split('\n')

    prompt = random.choice(prompts).split()
    prompt = prompt if len(prompt) <= 3 else prompt[:random.randint(1, 3)]
    prompt = ' '.join(prompt)

    if len(prompt) == 1:
        addition_object = codecs.open(path + "prompts_ideas.txt", "r", "utf_8_sig")
        # additions = addition_object.read().replace('\r', '').split('\n')
        addition = random.choice(prompts)

        prompt += ' ' + addition

    return prompt


def generate_text():
    while True:
        prompt = get_prompt()
        result = text_post_processing(pipe(prompt)[0]['generated_text'])[0]

        if len(result) > len(prompt):
            break
        else:
            print("Генерация продолжается")

    print("Prompt:", prompt, "Output:", result)

    if len(result) > 30 and len(result.split()) == 1:
        result = result[:30]

    return result


def add_shadow(text, draw, font, x, y, offset=3, shadow_color='black'):
    for off in range(offset):
        draw.text((x - off, y), text, font=font, fill=shadow_color)
        draw.text((x + off, y), text, font=font, fill=shadow_color)
        draw.text((x, y + off), text, font=font, fill=shadow_color)
        draw.text((x, y - off), text, font=font, fill=shadow_color)
        draw.text((x - off, y + off), text, font=font, fill=shadow_color)
        draw.text((x + off, y + off), text, font=font, fill=shadow_color)
        draw.text((x - off, y - off), text, font=font, fill=shadow_color)
        draw.text((x + off, y - off), text, font=font, fill=shadow_color)


def dynamic_text_position(text):
    # Select font size
    font_size = int(480 / len(text))

    # Select left-bottom of the text
    x = 15
    y = 240 - font_size

    return font_size, x, y


def put_text_on_image(image, text):
    draw = ImageDraw.Draw(image)

    # Select font
    fonts = os.listdir('fonts')
    font_choice = random.choice(fonts)

    # Select color
    color = (255, 255, 255)

    font_size, x, y = dynamic_text_position(text)

    # Import font
    with open('fonts/' + font_choice, "rb") as f:
        bytes_font = BytesIO(f.read())
    font = ImageFont.truetype(bytes_font, font_size if font_choice == "lobster.ttf" else font_size - 1)

    # Draw shadow
    add_shadow(text, draw, font, x, y)
    # Draw text
    draw.text((x, y), text, color, font=font)

    return image


@bot.message_handler(content_types=['text', 'image'])
def send_meme(message):
    if message.from_user.username is not None:
        # Skip if start message
        if message.text == '/start':
            return

        # Print info about new generation request into console
        print("Сообщение от пользователя:", message.from_user.username)
        print("Текст сообщения::", message.text)

        # Print info about new generation request into my tg chat

        bot.send_message(MY_CHAT_ID, f'Сообщение: {message.text}')
        bot.send_message(MY_CHAT_ID, f'От пользователя: {message.from_user.username}')

        # Generate image
        print('Начата генерация фото')
        # image = Image.fromarray(np.random.randint(0, 256, (IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8))
        image = generate_image()
        print('Фото сгенерировано')

        # Generate text
        print('Начата генерация текста')
        text = generate_text()
        print('Текст сгенерирован')

        # Create meme
        meme = put_text_on_image(image, text)

        # Save meme
        path = 'generated_data/generated_meme.png'
        save_image(meme, path)
        # save_image(image, path)
        print('Мем создан')

        # Send meme
        try:
            file = open(path, 'rb')
            try:
                if message.text != 'мем' and message.text != 'Мем':
                    bot.send_photo(MY_CHAT_ID, file, caption=message.text)
                else:
                    bot.send_photo(MY_CHAT_ID, file)
                print("Мем отправлен")
            except Exception as e:
                print("Error sending the photo")
                print(e)
        except Exception as e:
            print("Error reading the file")
            print(e)


# Loop for code execution
bot.infinity_polling(timeout=10, long_polling_timeout=5)
